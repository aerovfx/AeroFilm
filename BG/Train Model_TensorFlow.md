# ðŸ”· VÃ­ Dá»¥ 2: Train Model vá»›i TensorFlow trong Houdini  

## **BÆ°á»›c 1: CÃ i Ä‘áº·t TensorFlow**  
Náº¿u chÆ°a cÃ i Ä‘áº·t TensorFlow, má»Ÿ **Python Shell** trong Houdini vÃ  cháº¡y:  

```python
import pip
pip.main(["install", "tensorflow"])
```

---

## **BÆ°á»›c 2: Táº¡o Python Script Node trong TOP**  
TÆ°Æ¡ng tá»± nhÆ° trÃªn, táº¡o má»™t **Python Script** node vÃ  thÃªm cÃ¡c tham sá»‘:  

- **epochs** (int, máº·c Ä‘á»‹nh = 10)  
- **batch_size** (int, máº·c Ä‘á»‹nh = 16)  
- **learning_rate** (float, máº·c Ä‘á»‹nh = 0.001)  

Trong **Python Script**, dÃ¡n Ä‘oáº¡n code sau:  

```python
import tensorflow as tf
import numpy as np
import hou

# Láº¥y thÃ´ng sá»‘ tá»« Houdini
epochs = int(hou.pwd().parm("epochs").eval())
batch_size = int(hou.pwd().parm("batch_size").eval())
learning_rate = float(hou.pwd().parm("learning_rate").eval())

# Dummy dataset
x_train = np.random.randn(100, 1)
y_train = 3 * x_train + 2 + np.random.randn(100, 1) * 0.1

# XÃ¢y dá»±ng model Ä‘Æ¡n giáº£n
model = tf.keras.Sequential([
    tf.keras.layers.Dense(1, input_shape=(1,))
])

# Compile model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
              loss='mse')

# Training
model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)

# LÆ°u model
model.save(hou.pwd().parm("outputfile").eval())
print("Model training complete and saved.")
```

---

## **BÆ°á»›c 3: Cháº¡y Training**  
- ThÃªm tham sá»‘ **outputfile** Ä‘á»ƒ lÆ°u model (`output_model.h5`).  
- Nháº¥n **Cook Output Node** Ä‘á»ƒ cháº¡y training.  
